import functools
import os
import shutil
import typing as tp

import gin
import numpy as np
import scipy.sparse as sp
import tensorflow as tf
import tqdm
import wget
from tflo.matrix.core import CompositionMatrix, FullMatrix, Matrix
from tflo.matrix.extras import GatherMatrix, SparseMatrix

from ..utils.os_utils import get_dir
from ..utils.scipy_utils import (
    get_largest_component_indices,
    sp_to_tf,
    sparse_gather,
    sparse_normalize,
)
from .papers100m import get_papers100m_data
from .types import DataSplit, MLPData, PropTransitiveData, TransitiveData

register = functools.partial(gin.register, module="ppr_gnn.data.transitive")


@register
def basic_gnn_split(
    data: TransitiveData,
    *,
    renormalize: bool = False,
    device: str = "/cpu:0",
    dtype: tf.DType = tf.float32,
) -> DataSplit:
    """
    Get a basic GNN split with ((features, propagator), labels, weights).

    Note the returned weights are either 0 or 1 / num_labels. This makes them suitable
    for losses using `reduction="sum"` rather than `reduction="sum_over_batch_size"`.

    Args:
        data: base `TransitiveData`.
        renormalize: if True, identity is added to adjacency before symmetric
            normalization.
        device: device to perform tensorflow preprocessing on.
        dtype: data type of features / propagator / weights.

    Returns:
        `DataSplit` with each dataset having a single element
            `((features, propagator), labels, weights)`.
            - features: [num_nodes, num_features] dtype
            - propagator: [num_nodes, num_nodes] dtype
            - labels: [num_nodes] int64
            - weights: [num_nodes] dtype, sum(weights) == 1
    """
    adj = sparse_normalize(data.adjacency, renormalize=renormalize)
    with tf.device(device):
        adj = SparseMatrix(sp_to_tf(adj, dtype=dtype))
        if sp.issparse(data.node_features):
            features = sp_to_tf(data.node_features, dtype=dtype)
        else:
            features = tf.convert_to_tensor(data.node_features, dtype=dtype)
        labels = tf.convert_to_tensor(data.labels, tf.int64)

        def get_dataset(ids: tp.Optional[np.ndarray]) -> tp.Optional[tf.data.Dataset]:
            if ids is None:
                return None
            weights = np.zeros(labels.shape[0], dtype=np.float32)
            weights[ids] = 1 / ids.shape[0]
            weights = tf.convert_to_tensor(weights, dtype=dtype)
            return tf.data.Dataset.from_tensors(((features, adj), labels, weights))

        return DataSplit(
            get_dataset(data.train_ids),
            get_dataset(data.validation_ids),
            get_dataset(data.test_ids),
        )


@register
def cached_split(
    path: str,
    split_fn: tp.Optional[tp.Callable[[], DataSplit]] = None,
    overwrite: bool = False,
) -> DataSplit:
    """
    Get a `DataSplit` written to file.

    If the cache does not exist, it is generated by `split_fn`.

    Args:
        path: directory under which to save the data split.
        split_fn: function to create the `DataSplit` if `path` does not exist.
        overwrite: if True, the directory is deleted initially if it exists.

    Returns:
        `DataSplit`, whatever was previously saved to `path` using this function or
            was produced by `split_fn` if `path` was empty.
    """
    if overwrite and os.path.exists(path):
        shutil.rmtree(path)

    keys = ("train", "validation", "test")
    if os.path.exists(path):
        print(f"Loading cached split from {path}")

        def load(key: str) -> tp.Optional[tf.data.Dataset]:
            p = os.path.join(path, key)
            if os.path.exists(p):
                return tf.data.Dataset.load(p)
            return None

        split = DataSplit(*(load(key) for key in keys))
    else:
        if split_fn is None:
            raise ValueError(
                "`split_fn` must be provided if cache does not exist, and no directory "
                f"exists at {path}"
            )
        print("Computing split...")
        split = split_fn()
        print(f"Saving split to {path}...")
        for key, ds in zip(keys, split):
            if ds is not None:
                ds.save(os.path.join(path, key))

    return split


@register
def sub_batched_split(
    split: DataSplit, batch_size: int, shuffle_buffer: tp.Optional[int] = -1
) -> DataSplit:
    """
    Get a `DataSplit` made by breaking up `split`'s datasets into smaller batches.

    Args:
        split: `DataSplit` of datasets. If `batch_size != -1`, then each dataset must
            support `unbatch`.
        batch_size: resulting batch size. If -1, the split is returned unchanged.
        shuffle_buffer: size of the shuffle buffer applied to the training data. If -1,
            the entire dataset is shuffled. If `None`, no shuffling is applied.

    Returns:
        `DataSplit` with the same structure as `split` but with each dataset unbatched
            and rebatched (the train split is shuffled).
    """
    if batch_size == -1:
        return split

    def process(
        ds: tp.Optional[tf.data.Dataset], shuffle: bool = False
    ) -> tp.Optional[tf.data.Dataset]:
        if ds is None:
            return ds
        unbatched = ds.unbatch()
        if len(unbatched) <= batch_size:
            return ds
        else:
            ds = unbatched
        if shuffle and shuffle_buffer is not None:
            ds = ds.shuffle(len(ds) if shuffle_buffer == -1 else shuffle_buffer)
        ds = ds.batch(batch_size)
        return ds

    return DataSplit(
        process(split.train_data, True),
        process(split.validation_data),
        process(split.test_data),
    )


def _mlp_split(
    mlp_data: MLPData,
    train_ids: np.ndarray,
    validation_ids: tp.Optional[np.ndarray],
    test_ids: tp.Optional[np.ndarray],
) -> DataSplit:
    def get_dataset(ids: tp.Optional[np.ndarray]) -> tp.Optional[tf.data.Dataset]:
        if ids is None:
            return None
        with tf.device("/cpu:0"):
            example = (
                tf.convert_to_tensor(mlp_data.features[ids]),
                tf.convert_to_tensor(mlp_data.labels[ids]),
            )
            return tf.data.Dataset.from_tensors(example)

    return DataSplit(
        get_dataset(train_ids),
        get_dataset(validation_ids),
        get_dataset(test_ids),
    )


@register
def propagated_mlp_data(
    data: TransitiveData,
    propagator_fn: tp.Callable[[sp.spmatrix], Matrix],
    concat_original: bool = False,
    device: str = "/cpu:0",
) -> MLPData:
    """
    Get MLPData with features based on propagated input features.

    Propagated features are `propagator_fn(data.adjacency) @ data.node_features`.

    Args:
        data: input data.
        propagator_fn: function mapping adjacency -> propagation Matrix.
        concat_original: if True, returned features are the concatenation of
            `propagated_features` and `data.node_features`.
        device: where to compute tensorflow propagation.

    Returns:
        `MLPData`.
    """
    node_features = data.node_features
    if sp.issparse(node_features):
        node_features = node_features.todense()
    with tf.device(device):
        propagator = propagator_fn(data.adjacency)
        features = propagator @ tf.convert_to_tensor(node_features, propagator.dtype)
        features = features.numpy()
    if concat_original:
        features = np.concatenate((node_features, features), axis=-1)
    return MLPData(features, data.labels)


@register
def random_mlp_split(
    mlp_data: MLPData,
    train_samples_per_class: int,
    validation_samples_per_class: int,
    *,
    num_classes: tp.Optional[int] = None,
    seed: tp.Optional[int] = None,
    balanced: bool = True,
) -> DataSplit:
    """
    Get a `DataSplit` from `mlp_data` based on random ids.

    Args:
        mlp_data: `MLPData` with (features, labels) for every node.
        train_samples_per_class: number of training examples per class.
        validation_samples_per_class: number of validation examples per class.
        num_classes: number of classes.
        seed: random seed. Uses `np.random.randint(np.iinfo(np.int64).max)` if None.
        balanced: if True, exactly `train_samples_per_class` samples are selected for
            each class, otherwise a total of `train_samples_per_class * num_classes` is
            used. Same for validation.

    Returns:
        `DataSplit` computed with random train/validation ids, and the rest being test
        ids.
    """
    train_ids, validation_ids, test_ids = random_classification_ids(
        mlp_data.labels,
        samples_per_class=(train_samples_per_class, validation_samples_per_class),
        num_classes=num_classes,
        seed=seed,
        balanced=balanced,
    )

    return _mlp_split(mlp_data, train_ids, validation_ids, test_ids)


@register
def propagate_mlp_split(
    data: TransitiveData,
    propagator_fn: tp.Callable[[tf.SparseTensor], Matrix],
    *,
    concat_original: bool = False,
    device: str = "/cpu:0",
) -> DataSplit:
    """
    Get an MLP `DataSplit` using propagated features.

    This implementation explicitly computes `propagator @ node_features`, then slices
    out rows for each split associated with the relevant ids, i.e.

    ```python
    prop_feats = propagator_fn(adjacency) @ node_features
    train_feats = GatherMatrix(train_ids) @ prop_feats
    # ...
    ```

    This is efficient for datasets with a relatively small number of features per node.
    For datasets with a large number of features per node, see `propagate_mlp_split_v2`.

    Args:
        data: input data.
        propagator_fn: Function mapping adjacency `sp.spmatrix` to propagator
            `tflo.Matrix`.
        concat_original: if True, returned data has features made up of the
            concatenation of the original node_features and propagated node_features.
        device: device to perform tensorflow operations on.

    Returns:
        `DataSplit` with each dataset having a single element with structure
            (features, labels)
            - features: [num_labels, (2 if concat_original else 1) * num_features]
                same dtype as `propagator`
            - labels: [num_labels] int64
    """

    with tf.device(device):
        propagator = propagator_fn(data.adjacency)
        dtype = propagator.dtype

        features = data.node_features
        if sp.issparse(features):
            features = features.todense()

        input_features = tf.convert_to_tensor(features, dtype)
        features = propagator @ input_features

        if concat_original:
            features = tf.concat((input_features, features))

    return _mlp_split(
        MLPData(features.numpy(), data.labels),
        data.train_ids,
        data.validation_ids,
        data.test_ids,
    )


@register
def propagate_mlp_split_np(
    data: TransitiveData,
    propagator_fn: tp.Callable[[sp.spmatrix], tp.Callable[[np.ndarray], np.ndarray]],
    *,
    concat_original: bool = False,
    show_progress: bool = True,
    dtype: np.dtype = np.float32,
) -> DataSplit:
    """
    Get an MLP `DataSplit` using propagated features.

    Numpy equivalent of `propagate_mlp_split`. This generally takes longer but uses less
    memory.

    Args:
        data: input data.
        propagator_fn: Function mapping adjacency `sp.spmatrix` to propagator
            function `(ndarray) -> ndarray`.
        concat_original: if True, returned data has features made up of the
            concatenation of the original node_features and propagated node_features.
        show_progress: if True gives a progress bar for each propagator application.
        dtype: dtype of returned datasets' features.

    Returns:
        `DataSplit` with each dataset having a single element with structure
            (features, labels)
            - features: [num_labels, (2 if concat_original else 1) * num_features] dtype
            - labels: [num_labels] int64
    """
    propagator = propagator_fn(data.adjacency)
    num_features = data.node_features.shape[1]

    if concat_original:
        col_offset = num_features

        def get_empty_array(ids: np.ndarray):
            z = np.empty((ids.shape[0], num_features * 2), dtype=dtype)
            z[:, :num_features] = data.node_features[ids]
            return z

    else:
        col_offset = 0

        def get_empty_array(ids: np.ndarray):
            z = np.empty((ids.shape[0], num_features), dtype=dtype)
            return z

    train_data, validation_data, test_data = (
        get_empty_array(ids)
        for ids in (data.train_ids, data.validation_ids, data.test_ids)
    )
    if show_progress:
        it = tqdm.trange(num_features, desc="Propagating inputs...")
    else:
        it = range(num_features)

    for i in it:
        col = propagator(data.node_features[:, i])
        for dst, ids in (
            (train_data, data.train_ids),
            (validation_data, data.validation_ids),
            (test_data, data.test_ids),
        ):
            dst[:, col_offset + i] = col[ids]

    def get_dataset(features, ids):
        f = tf.convert_to_tensor(features, dtype)
        labels = tf.convert_to_tensor(data.labels[ids], tf.int64)
        dataset = tf.data.Dataset.from_tensors((f, labels))
        return dataset

    return DataSplit(
        get_dataset(train_data, data.train_ids),
        get_dataset(validation_data, data.validation_ids),
        get_dataset(test_data, data.test_ids),
    )


@register
def mlp_propagate_split(
    data: TransitiveData,
    propagator_fn: tp.Callable[[tf.SparseTensor], Matrix],
    *,
    preprocess_train: bool = False,
    preprocess_validation: bool = False,
    preprocess_test: bool = False,
    dtype: tf.DType = tf.float32,
    device: str = "/cpu:0",
) -> DataSplit:
    """
    Get a `DataSplit` suitable for models that use a single propagation.

    Args:
        data: input `TransitiveData`.
        propagator_fn: function mapping an adjacency `sp.spmatrix` to a
            propagation `tflo.Matrix`.
        preprocess_train: if True, the propagation matrix's rows associated with
            training nodes are preprocessed.
        preprocess_validation: if True, the propagation matrix's rows associated with
            validation nodes are preprocessed.
        preprocess_test: if True, the propagation matrix's rows associated with
            test nodes are preprocessed.
        dtype: tf.DType of the propagation matrix/node features.
        device: device on which to perform tensorflow operations during preprocessing.

    Returns:
        `DataSplit` with dataset elements ((features, propagator), labels).
            - features: [num_nodes, num_features] dtype
            - propagator: [num_labels, num_nodes] dtype
            - labels: [num_labels] int64
    """
    with tf.device(device):
        propagator = propagator_fn(data.adjacency)
        n = data.adjacency.shape[0]
        features = data.node_features
        if sp.issparse(features):
            features = sp_to_tf(features, dtype)
        else:
            features = tf.convert_to_tensor(features, dtype)

        def get_dataset(
            ids: tp.Optional[np.ndarray], preprocess: bool
        ) -> tp.Optional[tf.data.Dataset]:
            if ids is None:
                return None
            gather = GatherMatrix(tf.convert_to_tensor(ids, tf.int64), n, dtype=dtype)
            if isinstance(propagator, CompositionMatrix) and isinstance(
                propagator.operators[0], FullMatrix
            ):
                mat = CompositionMatrix(
                    (gather @ propagator.operators[0], *propagator.operators[1:])
                )
                if preprocess:
                    mat = mat.to_dense()
            else:
                if preprocess:
                    gather = gather.to_dense()
                mat = gather @ propagator
            labels = tf.convert_to_tensor(data.labels[ids], tf.int64)
            example = (features, mat), labels
            return tf.data.Dataset.from_tensors(example)

        return DataSplit(
            get_dataset(data.train_ids, preprocess_train),
            get_dataset(data.validation_ids, preprocess_validation),
            get_dataset(data.test_ids, preprocess_test),
        )


@register
def with_random_split_ids(
    data: tp.Union[TransitiveData, PropTransitiveData],
    train_samples_per_class: int,
    validation_samples_per_class: int,
    *,
    balanced: bool = True,
    seed: tp.Optional[int] = None,
) -> TransitiveData:
    """
    Get `TransitiveData` with ids re-randomized.

    Args:
        data: input data.
        train_samples_per_class: number of training samples per class.
        validation_samples_per_class: number of validation samples per class.
        seed: used for rng. Uses np.random.randint if missing.
        balanced: if True, uses examples the given number of samples per class. If
            False, the total number of samples in each split is the same, but the number
            of examples in each class may differ.

    Returns:
        A new `TransitiveData` object with the same `node_features`, `adjacency` and
        `labels` as the input `data` but with randomized `train_ids`, `validation_ids`
        and `test_ids`. `test_ids` is the set of ids once training and validation ids
        have been removed.
    """
    train_ids, validation_ids, test_ids = random_classification_ids(
        data.labels,
        (train_samples_per_class, validation_samples_per_class),
        balanced=balanced,
        seed=seed,
    )
    return data.rebuild(
        train_ids=train_ids, validation_ids=validation_ids, test_ids=test_ids
    )


def random_classification_ids(
    labels: np.ndarray,
    samples_per_class: tp.Sequence[int],
    num_classes: tp.Optional[int] = None,
    balanced: bool = True,
    seed: tp.Optional[int] = None,
) -> tp.Sequence[np.ndarray]:
    """
    Get randomly sampled indices.

    For example, if `num_nodes=200`, `num_classes=5` and `samples_per_class=[2, 3, 5]`,
    the returns splits will be of size `[10, 15, 25, 150]`.

    Args:
        labels: [num_nodes] in [0, num_classes)
        samples_per_class: [num_splits-1]
        balanced: if True, the returned splits (except for the last) will have the same
            number of each class present.
        seed: used in rng

    Returns:
        [num_splits] id arrays.
    """
    if num_classes is None:
        num_classes = int(labels.max()) + 1
    num_nodes = labels.shape[0]

    if seed is None:
        seed = np.random.randint(np.iinfo(np.int64).max)
    rng = np.random.default_rng(seed)

    def split(indices: np.ndarray, samples_per_class: tp.Sequence[int]):
        sections = np.cumsum(samples_per_class)
        rng.shuffle(indices)
        return np.split(indices, sections)

    if balanced:
        indices = [np.where(labels == i)[0] for i in range(num_classes)]
        class_indices = [split(ind, samples_per_class) for ind in indices]
        split_ids = (
            np.concatenate(split_indices, 0) for split_indices in zip(*class_indices)
        )
    else:
        # not balanced
        samples_per_class = [s * num_classes for s in samples_per_class]
        split_ids = split(np.arange(num_nodes), samples_per_class)

    split_ids = tuple(split_ids)
    for ids in split_ids:
        ids.sort()
    return split_ids


def apply(x, transforms: tp.Union[tp.Callable, tp.Iterable[tp.Callable], None]):
    """
    Apply `transforms` to `x`.

    If transforms is None, do nothing. If transforms is iterable, they are applied in
    series.

    Args:
        x: any input.
        transforms: callable, or iterable of callables to apply in sequence.

    Returns:
        Result of applying `transforms` to `x`.
    """
    if transforms is None:
        return x
    if callable(transforms):
        return transforms(x)
    for transform in transforms:
        x = transform(x)
    return x


@register
def transform_transitive_data(
    data: TransitiveData,
    largest_component_only: bool = False,
    data_transform: tp.Union[None, tp.Callable, tp.Sequence[tp.Callable]] = None,
    features_transform: tp.Union[None, tp.Callable, tp.Sequence[tp.Callable]] = None,
    adjacency_transform: tp.Union[None, tp.Callable, tp.Sequence[tp.Callable]] = None,
) -> TransitiveData:
    """
    Applies transformations to `data`.

    Transformations are applied as:
        - `get_largest_component` (if `largest_component_only`)
        - `data_transform`
        - `features_transform` (to `data.node_features`)
        - `adjacency_transform` (to `data.adjacency`)

    Args:
        data: base `TransitiveData`
        split_fn: function that maps `transformed_data -> DataSplit`. See e.g.
            `gnn_split_fn`, `mlp_split_fn`
        largest_component_only: if True, extracts the largest connected component before
          applying any other transformations.
        data_transform: a transform or series of transforms applied to `data`.
        features_transform: a transform or series of transforms applied to
            `data.node_features`.
        adjacency_transform: a transform or series of transforms applied to
            `data.adjaccency`.

    Returns:
        transformed `TransitiveData`.
    """
    if largest_component_only:
        data = get_largest_component(data)

    print("Applying data_transform")
    data = apply(data, data_transform)
    print("Applying features_transform")
    node_features = apply(data.node_features, features_transform)
    print("Applying adjacency_transform")
    adjacency = apply(data.adjacency, adjacency_transform)
    print("All transforms applied")
    return TransitiveData(
        node_features,
        adjacency,
        data.labels,
        data.train_ids,
        data.validation_ids,
        data.test_ids,
    )


def subgraph(data: TransitiveData, indices: np.ndarray) -> TransitiveData:
    """
    Get TransitiveData associated with the subgraph of given indices.

    Args:
        data: original `TransitiveData` with `in_size` nodes.
        indices: [out_size] int indices of nodes to extract.

    Returns:
        `TransitiveData` with `out_size` nodes and re-indexed adjacency/split ids.
    """
    adj: sp.coo_matrix = data.adjacency.tocoo()
    in_size = data.adjacency.shape[0]
    out_size = indices.size
    valid_nodes = np.zeros((in_size,), dtype=bool)
    valid_nodes[indices] = True
    valid_edges = np.logical_and(valid_nodes[adj.row], valid_nodes[adj.col])
    node_map = np.zeros((in_size,), dtype=indices.dtype)
    node_map[indices] = np.arange(out_size, dtype=node_map.dtype)

    def reindex(ids):
        if ids is None:
            return None
        return node_map[ids]

    row = reindex(adj.row[valid_edges])
    col = reindex(adj.col[valid_edges])
    adj = sp.coo_matrix((adj.data[valid_edges], (row, col)), shape=(out_size,) * 2)

    if sp.issparse(data.node_features):
        node_features = sparse_gather(data.node_features, indices)
    else:
        node_features = data.node_features[valid_nodes]

    return TransitiveData(
        node_features,
        adj,
        data.labels[valid_nodes],
        reindex(data.train_ids),
        reindex(data.validation_ids),
        reindex(data.test_ids),
    )


@register
def get_largest_component(
    data: TransitiveData, *, directed: bool = True, connection="weak"
) -> TransitiveData:
    """Get the `TransitiveData` associated with the largest connected component."""
    indices = get_largest_component_indices(
        data.adjacency, directed=directed, connection=connection
    )
    if indices is None:
        return data
    return subgraph(data, indices)


def _make_symmetric(
    row: np.ndarray, col: np.ndarray, shape: tp.Sequence[int], dtype=np.float32
) -> sp.spmatrix:
    n = shape[1]

    # isolate diagonals
    diag = row == col
    not_diag = np.logical_not(diag)
    d = row[diag].astype(np.int64) * (n + 1)
    row = row[not_diag]
    col = col[not_diag]
    del diag, not_diag

    # make lower triangular
    to_flip = col < row
    tmp = col[to_flip].copy()
    col[to_flip] = row[to_flip]
    row[to_flip] = tmp
    l = np.ravel_multi_index((row, col), shape)
    l.sort()
    l = np.unique(l)
    del to_flip, tmp, row, col

    # get upper triangular
    # row, col = l // n, l % n
    # u = col * n + row
    row, col = np.unravel_index(l, shape)
    u = np.ravel_multi_index((col, row), shape)

    # combine
    union = np.concatenate((l, d, u))
    union.sort()

    row, col = np.unravel_index(union, shape)
    data = np.ones((union.shape[0],), dtype=dtype)
    out = sp.coo_matrix((data, (row, col)), shape=shape)
    return out


def _load_dgl_graph(dgl_example, make_symmetric=False) -> sp.coo_matrix:
    r, c = (x.numpy() for x in dgl_example.edges())
    shape = (dgl_example.num_nodes(),) * 2
    if make_symmetric:
        # add symmetric edges
        return _make_symmetric(r, c, shape, dtype=np.float32)
    return sp.coo_matrix((np.ones((r.size,), dtype=np.float32), (r, c)), shape=shape)


@register
def get_data(name: str, *, make_symmetric: bool = True, **kwargs) -> TransitiveData:
    """
    Get data from any sources.

    See also: `ogbn_data`, `bojchevski_data`, `dgl_data`.

    Args:
        name: data name. Must be one supported by dgl, or start with `ogbn-` or
            `bojchevski/`
        make_symmetric: if True,
        **kwargs: passed to the relevant data function.

    Returns:
        TransitiveData

    Raises:
        NotImplementedError if `name` is not supported.
    """
    if name.startswith("ogbn-"):
        return ogbn_data(name, make_symmetric=make_symmetric, **kwargs)
    if name.startswith("bojchevski/"):
        return bojchevski_data(name, make_symmetric=make_symmetric, **kwargs)
    try:
        return dgl_data(name, make_symmetric=make_symmetric, **kwargs)
    except KeyError:
        pass
    raise NotImplementedError(f"Unrecognized data '{name}'")


def _load_dgl_example(
    dgl_example, make_symmetric: bool = False, sparse_features: bool = False
) -> TransitiveData:
    feat, label = (dgl_example.ndata[k].numpy() for k in ("feat", "label"))
    if sparse_features:
        feat = sp.coo_matrix(feat)
    train_ids, validation_ids, test_ids = (
        np.where(dgl_example.ndata[k].numpy())[0] if k in dgl_example.ndata else None
        for k in ("train_mask", "val_mask", "test_mask")
    )
    adj = _load_dgl_graph(dgl_example, make_symmetric=make_symmetric)
    return TransitiveData(feat, adj, label, train_ids, validation_ids, test_ids)


@register
def bojchevski_data(
    name: str,
    *,
    data_dir: tp.Optional[str] = None,
    make_symmetric: bool = True,
    sparse_features: tp.Optional[bool] = None,
) -> TransitiveData:
    """
    Get data sourced from Bojchevski et al.

    If the data does not exist on file it will be downloaded.

    Args:
        name: data name. Must be one of ('cora-full, 'pubmed', 'reddit', 'mag-coarse'),
            or one of these with "bojchevski/" prefixed.
        data_dir: directory to look for downloaded data or download data to.
        make_symmetric: if True, the return adjacency matrix will be symmetric.
        sparse_features: if True, the returned `node_features` will be sparse.
            If False, `node_features` will be dense. `None` will result in a
            sensible default. Note "reddit" does not support dense features.

    Returns:
        `TransitiveData`.
    """
    if name.startswith("bojchevski/"):
        name = name[len("bojchevski/") :]
    assert name in ("cora-full", "pubmed", "reddit", "mag-coarse"), name
    raw_dir = os.path.join(
        get_dir(data_dir, "PPR_GNN_DATA", "~/ppr-gnn-data"), "bojchevski"
    )
    os.makedirs(raw_dir, exist_ok=True)
    path = os.path.join(raw_dir, f"{name}.npz")
    if not os.path.exists(path):
        if name in ("cora-full", "pubmed"):
            url_name = name.replace(r"-", "_")
            url = (
                "https://github.com/TUM-DAML/pprgo_pytorch/raw/master/data/"
                f"{url_name}.npz"
            )
        elif name == "reddit":
            url = "https://figshare.com/ndownloader/files/23742119"
        elif name == "mag-coarse":
            url = "https://figshare.com/ndownloader/files/24045741"
        else:
            raise NotImplementedError(name)
        print(f"Downloading bojchevski/{name} data...")
        wget.download(url, path)
    data_dict = np.load(path)

    def load_csr(data, indices, indptr, shape, make_symmetric: bool):
        data = data.astype(np.float32)
        csr = sp.csr_matrix((data, indices, indptr), shape=shape)

        if make_symmetric:
            assert np.all(data == 1)
            coo: sp.coo_matrix = csr.tocoo()
            row = coo.row
            col = coo.col
            shape = coo.shape
            del coo, csr, data, indices, indptr
            return _make_symmetric(row, col, shape, dtype=np.float32)

        return csr

    if name in ("cora-full", "pubmed", "mag-coarse"):

        def load_sparse(prefix: str, make_symmetric: bool):
            indices, indptr, shape, data = (
                data_dict[f"{prefix}.{suffix}"]
                for suffix in (
                    "indices",
                    "indptr",
                    "shape",
                    "data",
                )
            )
            return load_csr(data, indices, indptr, shape, make_symmetric)

        adj = load_sparse("adj_matrix", make_symmetric=make_symmetric)
        features = load_sparse("attr_matrix", make_symmetric=False)
        if sparse_features == False:  # don't do this if sparse_features is None
            features = features.todense()
    elif name == "reddit":
        if sparse_features:
            raise ValueError("reddit features are dense, but sparse_features requested")
        features = data_dict["attr_matrix"]
        adj = load_csr(
            data_dict["adj_data"],
            data_dict["adj_indices"],
            data_dict["adj_indptr"],
            data_dict["adj_shape"],
            make_symmetric=make_symmetric,
        )
    else:
        raise NotImplementedError(name)
    labels = data_dict["labels"]
    return TransitiveData(features, adj, labels, None, None, None)


@register
def dgl_data(
    name: str,
    *,
    data_dir: tp.Optional[str] = None,
    make_symmetric: bool = True,
    sparse_features: bool = False,
) -> TransitiveData:
    """
    Get data sourced from Deep Graph Library (dgl).

    Args:
        name: data name. Must be one of (
                "cora", "pubmed", "citeseer", "amazon",
                "amazon", "coauthor", "coauthor", "cora-full"
            ).
        data_dir: `raw_dir` used in dgl constructors. If None, looks for an environment
            variable `DGL_DATA`, otherwise uses dgl default.
        make_symmetric: if True, the return adjacency matrix will be symmetric.
        sparse_features: if True, the returned `node_features` will be sparse.

    Returns:
        `TransitiveData`.
    """
    import dgl

    dgl_constructors = {
        "cora": dgl.data.CoraGraphDataset,
        "pubmed": dgl.data.PubmedGraphDataset,
        "citeseer": dgl.data.CiteseerGraphDataset,
        "amazon/computer": dgl.data.AmazonCoBuyComputerDataset,
        "amazon/photo": dgl.data.AmazonCoBuyPhotoDataset,
        "coauthor/physics": dgl.data.CoauthorPhysicsDataset,
        "coauthor/cs": dgl.data.CoauthorCSDataset,
        "cora-full": dgl.data.CoraFullDataset,
    }
    raw_dir = get_dir(data_dir, "DGL_DATA", None)
    ds = dgl_constructors[name](raw_dir=raw_dir)
    return _load_dgl_example(
        ds[0], make_symmetric=make_symmetric, sparse_features=sparse_features
    )


@register
def ogbn_data(
    name: str,
    *,
    data_dir: tp.Optional[str] = None,
    make_symmetric: bool = True,
) -> TransitiveData:
    """
    Get data sourced from Open Graph Benchmark (ogb).

    If the data does not exist on file it will be downloaded.

    Args:
        name: data name. Must be one of ('cora-full, 'pubmed', 'reddit', 'mag-coarse'),
            or one of these with "bojchevski/" prefixed.
        data_dir: directory to look for downloaded data or download data to.
        make_symmetric: if True, the return adjacency matrix will be symmetric.
        sparse_features: if True, the returned `node_features` will be sparse.

    Returns:
        `TransitiveData`.
    """
    import ogb.nodeproppred  # pylint: disable=import-outside-toplevel

    root_dir = get_dir(data_dir, "OGB_DATA", "~/ogb")
    if not name.startswith("ogbn-"):
        name = f"ogbn-{name}"

    if name.lower() == "ogbn-papers100m":
        return get_papers100m_data(ogb_dir=root_dir)

    print(f"Loading dgl {name}...")
    ds = ogb.nodeproppred.DglNodePropPredDataset(name, root=root_dir)
    print("Got base data. Initial preprocessing...")
    split_ids = ds.get_idx_split()
    train_ids, validation_ids, test_ids = (
        split_ids[n].numpy() for n in ("train", "valid", "test")
    )
    example, labels = ds[0]
    feats = example.ndata["feat"].numpy()
    labels = labels.numpy().squeeze(1)
    labels[np.isnan(labels)] = -1
    graph = _load_dgl_graph(example, make_symmetric=make_symmetric)
    print("Finished initial preprocessing")

    return TransitiveData(feats, graph, labels, train_ids, validation_ids, test_ids)


@register
def to_propagate_mlp_data(
    data: TransitiveData,
    propagator_fn: tp.Callable[[sp.spmatrix], tp.Callable[[np.ndarray], np.ndarray]],
) -> PropTransitiveData:
    """Convert `TransitiveData` data to `PropTransitiveData` with `propagator_fn`."""
    propagator = propagator_fn(data.adjacency)
    return PropTransitiveData(
        data.node_features,
        propagator,
        data.labels,
        data.train_ids,
        data.validation_ids,
        data.test_ids,
    )
